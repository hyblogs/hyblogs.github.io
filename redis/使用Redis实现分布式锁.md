# 使用 Redis 实现分布式锁





> &emsp;&emsp;在传统的项目中，实现分布式锁的方案一般是利用持久化数据库(如利用 MySQL 数据库中 InnoDB 引擎的**行锁**，或数据库事务，或 version 乐观锁)，当然这种做法也是可以满足一部项目的需求。但是如今互联网应用的量级已经呈现几何级别的爆发，再使用持久化数据库完成这一需求显然比较吃力了，需要利用诸如 Zookeeper、Redis、Memcached等更高效的分布式组件来实现分布式锁，这样可以提供高可用的更强壮的锁特性，并且支持丰富化的使用场景。接下来，咋们一起使用 Redis 来实现我们想要的分布式锁吧～

## 为什么要使用分布式锁

> &emsp;&emsp;为了防止分布式系统中的多个进程之间相互干扰，我们需要一种分布式协调技术来对这些进程进行调度。而这个分布式协调技术的核心就是这个分布式锁。在分布式场景下，有很多种情况都需要实现多节点的最终一致性。比如全局发号器，分布式事务等等。

## 分布式锁应具备哪些条件

### 互斥性

> 在任意时刻，只有一个客户端能持有锁。

### 不能死锁

> 客户端在持有锁的期间崩溃而没有主动解锁，也能保证后续其他客户端能加锁。

### 容错性

> 只要大部分的Redis节点正常运行，客户端就可以加锁和解锁。

## 普通实现 —— 单实例 Redis 环境

> 接下来将描述常规实现，以及带来的一系列相关问题，然后如何处理解决问题并进行优化。

### 加锁

> &emsp;&emsp;最简单的方法是使用 setnx 命令。key 是锁的唯一标识，按业务来决定命名。常见的一种秒杀场景：如果想要给其中一件商品的秒杀活动加锁，可以给 key 命名为 “lock_sale_商品ID” 。而 value 设置成什么呢？我们可以暂且设置成 1。对应的加锁伪代码如下：
> 
> ```
> setnx(lock_sale_商品ID，1)
> ```
> 
> 多个线程执行后会出现如下两种结果：
> 1、当一个线程执行 setnx 返回 1，说明 key 原本不存在，该线程成功得到了锁；
> 2、当一个线程执行 setnx 返回 0，说明 key 已经存在，该线程抢锁失败。

### 解锁

> &emsp;&emsp;加了锁自然得将其解开。当得到锁的线程执行完任务，需要释放锁，以便其他线程可以进入。释放锁的最简单方式就是执行 del 指令，伪代码如下：
> 
> ```
> del(lock_sale_商品ID)
> ```
> 
> 注：释放锁之后，其他线程就可以继续执行 setnx 命令来获得锁。

### 锁超时

> &emsp;&emsp;锁超时是什么意思呢？如果一个得到锁的线程在执行任务的过程中挂掉了，来不及显式地释放锁，这块资源将会永远被锁住(形成死锁)，别的线程再也别想进来。所以，setnx 的 key 必须设置一个超时时间，以保证在特殊情况下即使锁没有被显式释放，这把锁也要在一定时间后自动释放。setnx 不支持超时参数，所以需要额外的指令，伪代码如下：
> 
> ```
> expire(lock_sale_商品ID， 30)
> ```

### 存在问题一

> &emsp;&emsp;上面这种实现方案显然是不可行的，虽然 Redis 分别执行 setnx 和 expire 是原子性的，但是如果将两个原子性操作放一起分两步执行，那么它们并不是原子性了。可能会出现下面这种情形：
> &emsp;&emsp;分布式环境中，其中一个节点执行了 setnx 指令，但是还没来得及执行 expire 指令就节点异常崩溃了，此时刚才执行了 setnx 指令将导致对应的 key 是未设置超时时间的数据，这就造成了其他节点永远无法获取到锁，从而导致死锁。

### 解决改进问题一

> &emsp;&emsp;由于 setnx 指令本身是不支持传入超时时间参数的。所以，我们可以换用 set 指令增加可选参数进行处理，修改后的伪代码如下：
> 
> ```
> set(lock_sale_商品ID，1，30，NX)
> ```
> 
> 利用 set 指令支持多参数的特性替代 setnx + expire 就好了。

### 存在问题二

> &emsp;&emsp;你以为解决了上面的问题就完美了吗？其实不然，如果线程 A 获取到了锁，我们给获取到的锁设置了 30 秒的有效期，此时如果因为系统负载太高或其他原因导致线程 A 执行得非常慢，超过了 30 秒有效期，则线程 A 获取到的锁就被自动释放了，此时线程 B 刚好又获取到了这把锁，线程 B 又开始执行了。如果线程B在执行过程中，线程 A 执行完了删除了锁，那么线程 A 删除将是线程 B 刚获取到的锁，所以会导致后面的线程又获取到锁等等一系列问题。那么，我们该如何避免这种问题呢？

### 解决改进问题二

> &emsp;&emsp;问题二其实出现在 del 锁的这一步，所以我们可以从 del 锁的逻辑上进行改进，当我们需要 del 这把锁的时候，判断一下这把锁是否是我们自己加上的锁，如果不是我们自己加的锁则不能删除。
> 
> &emsp;&emsp;可以这样改进一下：对应锁的 value 要具有唯一性，可以使用 UUID.randomUUID().toString() 方法生成，用来标识这把锁是属于哪个请求加的，在解锁的时候就可以有依据；释放锁时要验证 value 值，防止误解锁。

### 守护线程保驾护航

> &emsp;&emsp;上面的这种处理方案有些不够完美，并发执行的时候还是会出现问题，那么如何处理并发问题呢？常见的处理方案是，在线程 A 获取到锁之后，可以新开一个**守护线程**，用来给线程 A 保驾护航，如果线程 A 执行比较耗时，本来设置的是 30 秒自动释放锁，但是线程 A 执行到 29 秒的时候依然没释放锁，还在忙忙碌碌的执行着。那么这时候守护进程可以为线程 A 的锁进行“续命”处理，执行 expire 指令延长加锁时间，比如设置再增长 30 秒有效期，只要线程 A 不执行完主动释放锁，那么守护线程就一直这样重复给它“续命”，直到现在 A 执行完成并释放锁后就可以关闭掉守护线程了。即使线程 A 在执行过程中宕机，守护线程也对应的挂掉了，那么也会根据有效期自动释放锁。

### 存在的风险

> &emsp;&emsp;在分布式环境下，Redis 对应的肯定不是单机，如果 Redis集群中存储锁对应 key 的那个节点挂了的话，就可能存在丢失锁的风险，导致出现多个客户端持有锁的情况，这样就不能实现资源的独享了。大概的场景如下：
> 
> - 1、线程 A 从 master 获取到锁；
> - 2、在 master 将锁同步到 slave 之前，master 宕掉了(Redis 的主从同步通常是异步的)。
> - 3、主从切换，slave 节点被晋级为 master 节点；
> - 4、线程 B 取得了同一个资源被客户端 A 已经获取到的另外一个锁。导致存在同一时刻不止一个线程获取到锁的情况。

## 集群多实例 —— 利用 Redlock 算法防止单点故障

> &emsp;&emsp;就如上文所述，在 Redis 集群环境中获取分布式锁如果遇到 master 宕机将会导致多个线程同一时刻获取到锁的异常情况发生。所以 Redis 官方推出了 **Redlock**。
> &emsp;&emsp;Redlock：全名叫做 Redis Distributed Lock；即使用redis 实现的分布式锁。详情的更多细节可以查看官网文档，地址如下：https://redis.io/topics/distlock

### 异常场景分析

> &emsp;&emsp;因为 redis 在进行主从复制时是异步完成的，比如在线程 A 获取锁后，主redis 复制数据到 从redis 过程中崩溃了，导致没有复制到 从redis 中，然后 从redis 选举出一个升级为 主redis，造成新的 主redis 没有线程 A 设置的锁，如果这时候线程 B 尝试获取锁，并且能够成功获取锁，导致互斥失效；
> 
> &emsp;&emsp;思考题：这个失败的原因是因为 从redis 立刻升级为 主redis，如果能够过TTL 时间再升级为 主redis(延迟升级)后，或者立刻升级为 主redis 但是过TTL 的时间后再执行获取锁的任务，就能成功产生互斥效果；是不是这样就能实现基于 redis主从 的 Redlock 了呢。

### 最低保证分布式锁的有效性及安全性的要求：

> - 1.互斥；任何时刻只能有一个client获取锁；
> - 2.释放死锁；即使锁定资源的服务崩溃或者分区，仍然能释放锁；
> - 3.容错性；只要多数redis节点（一半以上）在使用，client就可以获取和释放锁

### 利用 Redlock 算法有效防止单点故障

> &emsp;&emsp;假设有这样一个场景：有一个 redis cluster，有 5 个 redis master 实例。然后执行如下步骤获取一把锁：

> - 1、获取当前时间戳，单位是毫秒；
> - 2、client 尝试按照顺序使用相同的 key,value 获取所有 redis 服务的锁，在获取锁的过程中的获取时间比锁过期时间短很多，这是为了不要过长时间等待已经关闭的 redis 服务。并且试着获取下一个 redis 实例。轮流尝试在每个 master 节点上创建锁，过期时间较短，一般就几十毫秒；比如：TTL 为5s，设置获取锁最多用 1s，所以如果一秒内无法获取锁，就放弃获取这个锁，从而尝试获取下个锁。
> - 3.client 通过获取所有能获取的锁后的时间减去第一步的时间，这个时间差要小于 TTL 时间并且至少有 3 个(n / 2 + 1) redis 实例成功获取锁，才算真正的获取锁成功；
> - 4.如果成功获取锁，则锁的真正有效时间是 TTL 减去第三步的时间差的时间；比如：TTL 是5s,获取所有锁用了 2s，则真正锁有效时间为3s(其实应该再减去时钟漂移);
> - 5.如果客户端由于某些原因获取锁失败，便会开始解锁所有 redis 实例；因为可能已经获取了小于 3 个锁，必须释放，否则影响其他 client 获取锁；
> - 6、只要别人建立了一把分布式锁，你就得不断轮询去尝试获取锁。
>   ![Redlock](https://hyblogs.oss-cn-beijing.aliyuncs.com/hyblogs/image_1594280232179.png)
> 
> 概念解释：
> 1.TTL：Time To Live；指 redis key 的过期时间或有效生存时间；
> 2.clock drift：时钟漂移；指两个电脑间时间流速基本相同的情况下，两个电脑（或两个进程间）时间的差值；如果电脑距离过远会造成时钟漂移值过大。

### RedLock 算法是否是异步算法？？

> &emsp;&emsp;可以看成是同步算法；因为即使进程间（多个电脑间）没有同步时钟，但是每个进程时间流速大致相同；并且时钟漂移相对于 TTL 较小，可以忽略，所以可以看成同步算法；(不够严谨，算法上要算上时钟漂移，因为如果两个电脑在地球两端，则时钟漂移非常大)

### RedLock 失败重试

> &emsp;&emsp;当 client 不能获取锁时，应该在随机时间后重试获取锁；并且最好在同一时刻并发的把 set 命令发送给所有 redis 实例；而且对于已经获取锁的 client 在完成任务后要及时释放锁，这是为了节省时间；

### RedLock 释放锁

> &emsp;&emsp;由于释放锁时会判断这个锁的 value 是不是自己设置的，如果是才删除；所以在释放锁时非常简单，只要向所有实例都发出释放锁的命令，不用考虑能否成功释放锁；

### RedLock注意点(Safety arguments)

> - 1.先假设 client 获取所有实例，所有实例包含相同的 key 和过期时间(TTL)，但每个实例 set 命令时间不同导致不能同时过期，第一个 set 命令之前是 T1，最后一个 set 命令后为 T2，则此 client 有效获取锁的最小时间为 TTL - (T2 - T1) - 时钟漂移；
> - 2.对于以 N/2+ 1 (也就是一半以上)的方式判断获取锁成功，是因为如果小于一半判断为成功的话，有可能出现多个 client 都成功获取锁的情况，从而使锁失效；
> - 3.一个 client 锁定大多数实例耗费的时间大于或接近锁的过期时间，就认为锁无效，并且解锁这个 redis 实例(不执行业务)；只要在 TTL 时间内成功获取一半以上的锁便是有效锁，否则无效。

### 系统有活性的三个特征

> - 1.能够自动释放锁；
> - 2.在获取锁失败(不到一半以上)，或任务完成后 能够自动释放锁，不用等到其自动过期。
> - 3.在 client 重试获取到锁之前(第一次失败到第二次重试时间间隔)大于第一次获取锁消耗的时间；
> - 4.重试获取锁要有一定次数限制。

### RedLock 性能及崩溃恢复的相关解决方法

> - 1.如果 redis 没有持久化功能，在 clientA 获取锁成功后，所有redis 重启，clientB 能够再次获取到锁，这样违背了锁的排他互斥性;
> - 2.如果启动 AOF 永久化存储，事情会好些，例如：当我们重启 redis 后，由于 redis 过期机制是按照 unix 时间戳走的，所以在重启后，会按照规定的时间过期，不影响业务；但是由于 AOF 同步到磁盘的方式默认是每秒一 次，如果在一秒内断电，会导致数据丢失，立即重启会造成锁互斥性失效；但如果同步磁盘方式使用 Always (每一个写命令都同步到硬盘)造成性能急剧下降；所以在锁完全有效性和性能方面要有所取舍；
> - 3.有效解决既保证锁完全有效性及性能高效及即使断电情况的方法是 redis 同步到磁盘方式保持默认的每秒一次，在 redis 无论因为什么原因停掉后要等待 TTL 时间后再重启(学名:延迟重启)；缺点是在 TTL 时间内服务相当于暂停状态。

### 总结：

> - 1.TTL 时长要大于正常业务执行的时间 + 获取所有 redis 服务消耗时间 + 时钟漂移；
> - 2.获取 redis 所有服务消耗时间要远小于 TTL 时间，并且获取成功的锁个数要在总数的一半以上，即 N/2 + 1；
> - 3.尝试获取每个 redis 实例锁时的时间要远小于 TTL 时间；
> - 4.尝试获取所有锁失败后重新尝试一定要有一定的次数限制；
> - 5.在 redis 崩溃后(无论一个还是所有)，要延迟 TTL 时间重启 redis；
> - 6.在实现多 redis 节点时要结合单节点分布式锁算法共同实现。
> 
> 感兴趣的同学，建议阅读官方文档英文版～

## 集群多实例 —— 利用 Redisson 实现分布式锁

> &emsp;&emsp;Redisson 是一个在 Redis 的基础上实现的 Java 驻内存数据网格(In-Memory Data Grid)。它不仅提供了一系列的分布式的 Java 常用对象，还实现了可重入锁(Reentrant Lock)、公平锁(Fair Lock)、联锁(MultiLock)、 红锁(RedLock)、 读写锁(ReadWriteLock)等，还提供了许多分布式服务。
> 
> &emsp;&emsp;Redisson 提供了使用 Redis 的最简单和最便捷的方法。Redisson 的宗旨是促进使用者对 Redis 的关注分离(Separation of Concern)，从而让使用者能够将精力更集中地放在处理业务逻辑上。

### Redisson 分布式重入锁用法

> Redisson 支持单点模式、主从模式、哨兵模式、集群模式，这里以单点模式为例：

```java
// 1.构造redisson实现分布式锁必要的Config
Config config = new Config();
config.useSingleServer().setAddress("redis://127.0.0.1:6379").setPassword("123456").setDatabase(0);
// 2.构造RedissonClient
RedissonClient redissonClient = Redisson.create(config);
// 3.获取锁对象实例（无法保证是按线程的顺序获取到）
RLock rLock = redissonClient.getLock(lockKey);
try {
    /**
     * 4.尝试获取锁
     * waitTimeout 尝试获取锁的最大等待时间，超过这个值，则认为获取锁失败
     * leaseTime   锁的持有时间,超过这个时间锁会自动失效（值应设置为大于业务处理的时间，确保在锁有效期内业务能处理完）
     */
    boolean res = rLock.tryLock((long)waitTimeout, (long)leaseTime, TimeUnit.SECONDS);
    if (res) {
        //成功获得锁，在这里处理业务
    }
} catch (Exception e) {
    throw new RuntimeException("aquire lock fail");
} finally {
    //无论如何, 最后都要解锁
    rLock.unlock();
}
```

### 加锁流程图

![lock](https://hyblogs.oss-cn-beijing.aliyuncs.com/hyblogs/image_1594281673058.png)

### 解锁流程图

![unlock](https://hyblogs.oss-cn-beijing.aliyuncs.com/hyblogs/image_1594281694021.png)

### 总结

> &emsp;&emsp;我们可以看到，RedissonLock 是可重入的，并且考虑了失败重试，可以设置锁的最大等待时间，在实现上也做了一些优化，减少了无效的锁申请，提升了资源的利用率。
> 
> &emsp;&emsp;需要特别注意的是，RedissonLock 同样没有解决节点挂掉的时候，存在丢失锁的风险的问题。而现实情况是有一些场景无法容忍的，所以 Redisson 提供了实现了 redlock 算法的 RedissonRedLock，RedissonRedLock 真正解决了单点失败的问题，代价是需要额外的为 RedissonRedLock 搭建 Redis 环境。
> 
> &emsp;&emsp;所以，如果业务场景可以容忍这种小概率的错误，则推荐使用 RedissonLock，如果无法容忍，则推荐使用 RedissonRedLock。

#### 参考说明

> 以上内容参考来源如下：
> https://mp.weixin.qq.com/s/kjbVXruP_ed2sr3DvuMWNw
> https://www.jianshu.com/p/a1ebab8ce78a
> https://www.cnblogs.com/rgcLOVEyaya/p/RGC_LOVE_YAYA_1003days.html
